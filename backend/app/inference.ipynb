{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:46:53.559364Z",
     "iopub.status.busy": "2025-05-24T08:46:53.559098Z",
     "iopub.status.idle": "2025-05-24T08:47:01.715708Z",
     "shell.execute_reply": "2025-05-24T08:47:01.714837Z",
     "shell.execute_reply.started": "2025-05-24T08:46:53.559333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:48:10.944655Z",
     "iopub.status.busy": "2025-05-24T08:48:10.944196Z",
     "iopub.status.idle": "2025-05-24T08:48:42.416019Z",
     "shell.execute_reply": "2025-05-24T08:48:42.415017Z",
     "shell.execute_reply.started": "2025-05-24T08:48:10.944617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\react_native\\draft - Copy\\.venv\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('../data/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:49:23.395320Z",
     "iopub.status.busy": "2025-05-24T08:49:23.395036Z",
     "iopub.status.idle": "2025-05-24T08:49:23.401871Z",
     "shell.execute_reply": "2025-05-24T08:49:23.400070Z",
     "shell.execute_reply.started": "2025-05-24T08:49:23.395300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 500\n",
    "\n",
    "def preprocess_input(text, vocab, max_length=MAX_LENGTH):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    sequence = [vocab[word] for word in tokens if word in vocab]\n",
    "    padded = sequence[:max_length] + [0] * (max_length - len(sequence))\n",
    "    return torch.tensor([padded], dtype=torch.long)  # Shape: (1, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:51:19.342987Z",
     "iopub.status.busy": "2025-05-24T08:51:19.342686Z",
     "iopub.status.idle": "2025-05-24T08:51:19.357882Z",
     "shell.execute_reply": "2025-05-24T08:51:19.356635Z",
     "shell.execute_reply.started": "2025-05-24T08:51:19.342965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNPersonalityModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, output_dim=16, embedding_matrix=None):\n",
    "        super(CNNPersonalityModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False) if embedding_matrix is not None else nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=fs, padding=1)\n",
    "            for fs in (3, 4, 5)\n",
    "        ])\n",
    "        self.fc1 = nn.Linear(128 * len((3, 4, 5)), 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(128 * len((3, 4, 5)))\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = [torch.relu(conv(x)).max(dim=2)[0] for conv in self.convs]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features before the final classification layer\"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = [torch.relu(conv(x)).max(dim=2)[0] for conv in self.convs]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x  # Return features from the layer before fc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:51:22.440645Z",
     "iopub.status.busy": "2025-05-24T08:51:22.439535Z",
     "iopub.status.idle": "2025-05-24T08:51:22.450892Z",
     "shell.execute_reply": "2025-05-24T08:51:22.449629Z",
     "shell.execute_reply.started": "2025-05-24T08:51:22.440602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GRUPersonalityClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.gru2 = nn.GRU(\n",
    "            input_size=hidden_dim * 2,  # because first GRU is bidirectional\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        outputs, hidden = self.gru1(embedded)\n",
    "        outputs, hidden = self.gru2(outputs)\n",
    "        \n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        x = self.fc1(hidden)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features before the final classification layer\"\"\"\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        outputs, hidden = self.gru1(embedded)\n",
    "        outputs, hidden = self.gru2(outputs)\n",
    "        \n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        x = self.fc1(hidden)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x  # Return features before the final fc2 layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:51:25.063028Z",
     "iopub.status.busy": "2025-05-24T08:51:25.062071Z",
     "iopub.status.idle": "2025-05-24T08:51:25.073637Z",
     "shell.execute_reply": "2025-05-24T08:51:25.072639Z",
     "shell.execute_reply.started": "2025-05-24T08:51:25.062998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StackedMBTIModel(nn.Module):\n",
    "    def __init__(self, cnn_model_path, gru_model_path, output_dim):\n",
    "        super(StackedMBTIModel, self).__init__()\n",
    "\n",
    "        self.cnn_model = self._load_model(cnn_model_path)\n",
    "        self.gru_model = self._load_model(gru_model_path)\n",
    "\n",
    "        self._freeze_model_params(self.cnn_model)\n",
    "        self._freeze_model_params(self.gru_model)\n",
    "        \n",
    "        self.cnn_feature_dim = 64\n",
    "        self.gru_feature_dim = 256\n",
    "        \n",
    "        self.meta_learner = nn.Sequential(\n",
    "            nn.Linear(self.cnn_feature_dim + self.gru_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def _load_model(self, model_path):\n",
    "        try:\n",
    "            model = torch.load(model_path, weights_only=False)\n",
    "            model.eval()  # Set to evaluation mode\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from {model_path}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _freeze_model_params(self, model):\n",
    "        \"\"\"Freeze parameters of a model to prevent them from being updated during training\"\"\"\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the stacked model\n",
    "        \n",
    "        Args:\n",
    "            x: Input data (text sequences)\n",
    "            \n",
    "        Returns:\n",
    "            MBTI class predictions\n",
    "        \"\"\"\n",
    "        # Get features from base models\n",
    "        with torch.no_grad():  # No need to compute gradients for frozen models\n",
    "            cnn_features = self.cnn_model.get_features(x)\n",
    "            gru_features = self.gru_model.get_features(x)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((cnn_features, gru_features), dim=1)\n",
    "        \n",
    "        # Pass through meta-learner\n",
    "        output = self.meta_learner(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:56:05.866378Z",
     "iopub.status.busy": "2025-05-24T08:56:05.866046Z",
     "iopub.status.idle": "2025-05-24T08:56:05.926748Z",
     "shell.execute_reply": "2025-05-24T08:56:05.925659Z",
     "shell.execute_reply.started": "2025-05-24T08:56:05.866355Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedMBTIModel(\n",
       "  (cnn_model): CNNPersonalityModel(\n",
       "    (embedding): Embedding(20001, 128)\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(1,))\n",
       "      (2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (fc1): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (fc4): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (batch_norm1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gru_model): GRUPersonalityClassifier(\n",
       "    (embedding): Embedding(20001, 128, padding_idx=0)\n",
       "    (gru1): GRU(128, 256, batch_first=True, bidirectional=True)\n",
       "    (gru2): GRU(512, 256, batch_first=True, bidirectional=True)\n",
       "    (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc2): Linear(in_features=256, out_features=16, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (meta_learner): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('../models/stacked_mbti_model.pt', map_location=torch.device(\"cpu\"), weights_only=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:56:09.239710Z",
     "iopub.status.busy": "2025-05-24T08:56:09.239264Z",
     "iopub.status.idle": "2025-05-24T08:56:09.245299Z",
     "shell.execute_reply": "2025-05-24T08:56:09.244346Z",
     "shell.execute_reply.started": "2025-05-24T08:56:09.239685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_mbti(text):\n",
    "    input_tensor = preprocess_input(text, vocab)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_index = torch.argmax(output, dim=1).item()\n",
    "        predicted_type = label_encoder.classes_[predicted_index]\n",
    "    return predicted_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zahra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\zahra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T08:56:11.771955Z",
     "iopub.status.busy": "2025-05-24T08:56:11.771656Z",
     "iopub.status.idle": "2025-05-24T08:56:12.256049Z",
     "shell.execute_reply": "2025-05-24T08:56:12.255195Z",
     "shell.execute_reply.started": "2025-05-24T08:56:11.771938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted MBTI: INTP\n"
     ]
    }
   ],
   "source": [
    "text = \"I love spending time reading books and reflecting on life.\"\n",
    "print(\"Predicted MBTI:\", predict_mbti(text))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6879225,
     "sourceId": 11043436,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7500909,
     "sourceId": 11930894,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 355337,
     "modelInstanceId": 334323,
     "sourceId": 409178,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 343621,
     "modelInstanceId": 322930,
     "sourceId": 392202,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
